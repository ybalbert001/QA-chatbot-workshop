import json
import logging
import os
from botocore import config
from botocore.exceptions import ClientError
from datetime import datetime, timedelta
import boto3
import time
import requests
import uuid
from transformers import AutoTokenizer
from enum import Enum


logger = logging.getLogger()
logger.setLevel(logging.INFO)
sm_client = boto3.client("sagemaker-runtime")
llm_endpoint = 'bloomz-7b1-mt-2023-04-19-09-41-24-189-endpoint'

class ErrorCode:
    DUPLICATED_INDEX_PREFIX = "DuplicatedIndexPrefix"
    DUPLICATED_WITH_INACTIVE_INDEX_PREFIX = "DuplicatedWithInactiveIndexPrefix"
    OVERLAP_INDEX_PREFIX = "OverlapIndexPrefix"
    OVERLAP_WITH_INACTIVE_INDEX_PREFIX = "OverlapWithInactiveIndexPrefix"
    INVALID_INDEX_MAPPING = "InvalidIndexMapping"


class APIException(Exception):
    def __init__(self, message, code: str = None):
        if code:
            super().__init__("[{}] {}".format(code, message))
        else:
            super().__init__(message)


def handle_error(func):
    """Decorator for exception handling"""

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except APIException as e:
            logger.exception(e)
            raise e
        except Exception as e:
            logger.exception(e)
            raise RuntimeError(
                "Unknown exception, please check Lambda log for more details"
            )

    return wrapper

# kendra


def query_kendra(Kendra_index_id="", lang="zh", search_query_text="what is s3?", Kendra_result_num=3):
    # è¿æ¥åˆ°Kendra
    client = boto3.client('kendra')

    # æ„é€ KendraæŸ¥è¯¢è¯·æ±‚
    query_result = client.query(
        IndexId=Kendra_index_id,
        QueryText=search_query_text,
        AttributeFilter={
            "EqualsTo": {
                "Key": "_language_code",
                "Value": {
                    "StringValue": lang
                }
            }
        }
    )
    # print(query_result['ResponseMetadata']['HTTPHeaders'])
    # kendra_took = query_result['ResponseMetadata']['HTTPHeaders']['x-amz-time-millis']
    # åˆ›å»ºä¸€ä¸ªç»“æœåˆ—è¡¨
    results = []

    # å°†æ¯ä¸ªç»“æœæ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­
    for result in query_result['ResultItems']:
        # åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥ä¿å­˜æ¯ä¸ªç»“æœ
        result_dict = {}

        result_dict['title'] = result['DocumentTitle']['Text']
        result_dict['id'] = result['DocumentId']

        # å¦‚æœæœ‰å¯ç”¨çš„æ€»ç»“
        if 'DocumentExcerpt' in result:
            result_dict['excerpt'] = result['DocumentExcerpt']['Text']
        else:
            result_dict['excerpt'] = ''

        # å°†ç»“æœæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        results.append(result_dict)

    # è¾“å‡ºç»“æœåˆ—è¡¨
    return {"kendra_results": results[:Kendra_result_num]}


# AOS
def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):
    parameters = {
        # "early_stopping": True,
        # "length_penalty": 2.0,
        "max_new_tokens": 50,
        "temperature": 0,
        "min_length": 10,
        "no_repeat_ngram_size": 2,
    }

    response_model = sm_client.invoke_endpoint(
        EndpointName=endpoint_name,
        Body=json.dumps(
            {
                "inputs": questions,
                "parameters": parameters
            }
        ),
        ContentType="application/json",
    )
    json_str = response_model['Body'].read().decode('utf8')
    json_obj = json.loads(json_str)
    embeddings = json_obj['sentence_embeddings']
    return embeddings

def search_using_aos_knn(q_embedding, hostname, index, source_includes, size):
    # awsauth = (username, passwd)
    # print(type(q_embedding))
    headers = {"Content-Type": "application/json"}
    query = {
        "size": size,
        "query": {
            "knn": {
                "embedding": {
                    "vector": q_embedding,
                    "k": size
                }
            }
        }
    }
    r = requests.post("https://"+hostname + "/" + index +
                        '/_search', headers=headers, json=query)
    return r.text

# def query_opensearch(api_url, query_type, query_params):
#     headers = {'Content-Type': 'application/json'}
#     if query_type == 'vector':  # å‘é‡æŸ¥è¯¢
#         query_json = {
#             "size": query_params['size'],
#             "query": {
#                 "knn": {
#                     query_params['field']: {
#                         "vector": query_params['vector_values'],
#                         "k": query_params['k']
#                     }
#                 }
#             }
#         }
#     elif query_type == 'match': # å­—ç¬¦ä¸²åŒ¹é…æŸ¥è¯¢
#         query_json = {
#             "size": query_params['size'],
#             "query": {
#                 "match": {
#                     query_params['field']: query_params['match_keyword']
#                 }
#             }
#         }

#     response = requests.post(api_url, headers=headers, data=json.dumps(query_json))
#     if response.status_code == 200:
#         # è¯·æ±‚æˆåŠŸ
#         response_json = json.loads(response.text)
#         hits = response_json['hits']['hits']
#         return [hit['_source'] for hit in hits]
#     else:
#         # è¯·æ±‚å¤±è´¥
#         print(f"è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
#         return []


# def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name, parameters):
#     response_model = sm_client.invoke_endpoint(
#         EndpointName=endpoint_name,
#         Body=json.dumps(
#             {
#                 "inputs": questions,
#                 "parameters": parameters
#             }
#         ),
#         ContentType="application/json",
#     )
#     json_str = response_model['Body'].read().decode('utf8')
#     json_obj = json.loads(json_str)
#     embeddings = json_obj['sentence_embeddings']
#     return embeddings


# def search_using_aos_knn(q_embedding, hostname, index, source_includes, size):
#     # awsauth = (username, passwd)
#     print(type(q_embedding))
#     query = {
#         "size": size,
#         "query": {
#             "knn": {
#                 "sentence_vector": {
#                     "vector": q_embedding,
#                     "k": size
#                 }
#             }
#         }
#     }
#     r = requests.post("https://"+hostname + "/" + index +
#                       '/_search', headers=headers, json=query)
#     return r.text

# DDB


def get_session(session_id):

    table_name = "chatbot-session"
    dynamodb = boto3.resource('dynamodb')

    # table name
    table = dynamodb.Table(table_name)
    operation_result = ""

    response = table.get_item(Key={'session-id': session_id})

    if "Item" in response.keys():
        print("****** " + response["Item"]["content"])
        operation_result = response["Item"]["content"]
    else:
        print("****** No result")
        operation_result = "none"

    return operation_result


# param:    session_id
#           question
#           answer
# return:   success
#           failed
def update_session(session_id, question, answer):

    table_name = "chatbot-session"
    dynamodb = boto3.resource('dynamodb')

    # table name
    table = dynamodb.Table(table_name)
    operation_result = ""
    content = ""

    response = table.get_item(Key={'session-id': session_id})

    if "Item" in response.keys():
        print("****** " + response["Item"]["content"])
        content = response["Item"]["content"] + ", "
    else:
        print("****** No result")
        content = ""

    content = content + "('" + question + "', '" + answer + "')"

    # inserting values into table
    response = table.put_item(
        Item={
            'session-id': session_id,
            'content': content
        }
    )

    if "ResponseMetadata" in response.keys():
        if response["ResponseMetadata"]["HTTPStatusCode"] == 200:
            operation_result = "success"
        else:
            operation_result = "failed"
    else:
        operation_result = "failed"

    return operation_result


# LLM
Game_FreeChat_Example="""ç©å®¶è¾“å…¥: ä»‹ç»ä¸€ä¸‹è”ç›Ÿ?
è¾“å‡º: åŠŸèƒ½ç›¸å…³

ç©å®¶è¾“å…¥: ä»‹ç»ä¸€ä¸‹å¼ºåŒ–éƒ¨ä»¶?
è¾“å‡º: åŠŸèƒ½ç›¸å…³

ç©å®¶è¾“å…¥:æˆ‘ç©ä½ ä»¬çš„æ¸¸æˆå¯¼è‡´å¤±æ‹äº†
è¾“å‡º: åŠŸèƒ½æ— å…³

ç©å®¶è¾“å…¥: æˆ‘è¦ç»™ä½ ä»¬æä¸ªå»ºè®®ï¼
è¾“å‡º: åŠŸèƒ½æ— å…³

ç©å®¶è¾“å…¥:æ€ä¹ˆæ‰èƒ½è¿ç§»åŒºæœï¼Ÿ
è¾“å‡º: åŠŸèƒ½ç›¸å…³"""

Game_Intention_Classify_Prompt = """
ä»»åŠ¡å®šä¹‰: åˆ¤æ–­ç©å®¶è¾“å…¥æ˜¯å¦åœ¨è¯¢é—®æ¸¸æˆåŠŸèƒ½ç›¸å…³çš„é—®é¢˜, å›ç­” "åŠŸèƒ½ç›¸å…³" æˆ–è€… "åŠŸèƒ½æ— å…³".

{fewshot}

ç©å®¶è¾“å…¥:{question}
è¾“å‡º: """

Game_Knowledge_QA_Prompt = """
Jarvis æ˜¯ä¸€ä¸ªæ¸¸æˆæ™ºèƒ½å®¢æœï¼Œèƒ½å¤Ÿå›ç­”ç©å®¶çš„å„ç§é—®é¢˜ï¼Œä»¥åŠé™ªç”¨æˆ·èŠå¤©ï¼Œæ¯”å¦‚

{fewshot}

ç©å®¶:{question}
Jarvis:"""

Game_Free_Chat_Prompt = """
Jarvis æ˜¯ä¸€ä¸ªæ¸¸æˆæ™ºèƒ½å®¢æœï¼Œèƒ½å¤Ÿå›ç­”ç©å®¶çš„å„ç§é—®é¢˜ï¼Œä»¥åŠé™ªç”¨æˆ·èŠå¤©ï¼Œæ¯”å¦‚

{fewshot}

ç©å®¶: {question}
Jarvis:"""


def Generate(smr_client, llm_endpoint, prompt):
    parameters = {
        # "early_stopping": True,
        "length_penalty": 100.0,
        "max_new_tokens": 200,
        "temperature": 0,
        "min_length": 20,
        "no_repeat_ngram_size": 200
    }

    response_model = smr_client.invoke_endpoint(
        EndpointName=llm_endpoint,
        Body=json.dumps(
            {
                "inputs": prompt,
                "parameters": parameters
            }
        ),
        ContentType="application/json",
    )

    return response_model['Body'].read().decode('utf8')


class QueryType(Enum):
    KeyWordOnly = "Keyword"   #ç”¨æˆ·ä»…ä»…è¾“å…¥äº†ä¸€äº›å…³é”®è¯ï¼ˆ2 token)
    NormalQuery = "KnowledgeQuery"   #ç”¨æˆ·è¾“å…¥çš„éœ€è¦å‚è€ƒçŸ¥è¯†åº“æœ‰å…³æ¥å›ç­”
    NonKnowledge = "Conversation"  #ç”¨æˆ·è¾“å…¥çš„æ˜¯è·ŸçŸ¥è¯†åº“æ— å…³çš„é—®é¢˜


def intention_classify(post_text, prompt_template, few_shot_example):
    prompt = prompt_template.format(
        fewshot=few_shot_example, question=post_text)
    print(prompt)
    result = Generate(sm_client, llm_endpoint, prompt)
    print(result)
    len_prompt = len(prompt)
    return result[len_prompt:]


def prompt_build(post_text, opensearch_respose, opensearch_knn_respose, kendra_respose, conversations, tokenizer):
    """
    Merge all retrieved related document paragraphs into a single prompt
    opensearch_respose: [{"score" : 0.7, "doc": "....", "doc_type": "Q|A|P"}]
    opensearch_knn_respose: [
        {"score" : 0.7, "doc": "....", "doc_type": "Q|A|P"}]
    kendra_respose: [{"score" : 0.7, "doc": "....", "doc_type": "Q|A|P"}]
    conversations: [ ("Q1", "A1"), ("Q1", "A1"), ...]
    tokenizer: which aim to calculate length of query's token
    return: prompt string
    """
    q_type = QueryType.NormalQuery
    prompt_context = ""


    # tokens = tokenizer.encode(post_text)
    if (len(post_text) <= 2):
        q_type = QueryType.NormalQuery
        if opensearch_respose:
            prompt_context = opensearch_respose[-1]
    else:  # NormalQuery
        if "åŠŸèƒ½æ— å…³" == intention_classify(post_text, Game_Intention_Classify_Prompt, Game_FreeChat_Example):
            prompt_context = "\n".join(conversations)
            q_type = QueryType.NonKnowledge
        else:  # åŠŸèƒ½ç›¸å…³ï¼Œ
            # Combine opensearch_knn_respose and kendra_respose
            recall_dict = {}
            # for recall_item in opensearch_respose:
            #     if recall_item["doc"] in recall_item.keys():
            #         if recall_item["score"] > recall_dict[recall_item["doc"]]:
            #             recall_dict[recall_item["doc"]] = recall_item["score"]
            #     else:
            #         recall_dict[recall_item["doc"]] = recall_item["score"]

            for recall_item in opensearch_knn_respose:
                if recall_item["doc_type"] != 'P':
                    continue

                if recall_item["doc"] in recall_item.keys():
                    if recall_item["score"] > recall_dict[recall_item["doc"]]:
                        recall_dict[recall_item["doc"]] = recall_item["score"]
                else:
                    recall_dict[recall_item["doc"]] = recall_item["score"]

#             for recall_item in kendra_respose:
#                 if recall_item["doc"] in recall_item.keys():
#                     if recall_item["score"] > recall_dict[recall_item["doc"]]:
#                         recall_dict[recall_item["doc"]] = recall_item["score"]
#                 else:
#                     recall_dict[recall_item["doc"]] = recall_item["score"]

            example_list = [k for k, v in sorted(
                recall_dict.items(), key=lambda item: item[1])]
            q_type = QueryType.NormalQuery
            prompt_context = '\n\n'.join(example_list)

        final_prompt = Game_Knowledge_QA_Prompt.format(
            fewshot=prompt_context, question=post_text)
        final_prompt = final_prompt.replace(
            "Question", "ç©å®¶").replace("Answer", "Jarvis")

        json_obj = {
            "query": post_text,
            "opensearch_doc":  opensearch_respose,
            "opensearch_knn_doc":  opensearch_knn_respose,
            "kendra_doc": kendra_respose,
            "detect_query_type": str(q_type),
            "LLM_input": final_prompt
        }

        return q_type, final_prompt, json_obj

def main_entry(session_id:str, query_input:str, embedding_model_endpoint:str, llm_model_endpoint:str, aos_endpoint:str, aos_index:str, aos_knn_field:str, aos_result_num:int, kendra_index_id:str, kendra_result_num:int):
    """
    Entry point for the Lambda function.

    Parameters:
        session_id (str): The ID of the session.
        query_input (str): The query input.
        embedding_model_endpoint (str): The endpoint of the embedding model.
        llm_model_endpoint (str): The endpoint of the language model.
        aos_endpoint (str): The endpoint of the AOS engine.
        aos_index (str): The index of the AOS engine.
        aos_knn_field (str): The knn field of the AOS engine.
        aos_result_num (int): The number of results of the AOS engine.
        kendra_index_id (str): The ID of the Kendra index.
        kendra_result_num (int): The number of results of the Kendra Service.

    return: answer(str)
    """
    sm_client = boto3.client("sagemaker-runtime")
    
    # 1. get_session
    session_data = get_session(session_id=session_id)

    # 2. get kendra recall 
    kendra_respose = query_kendra(kendra_index_id, "zh", query_input, kendra_result_num)

    # 3. get AOS knn recall 
    query_embedding = get_vector_by_sm_endpoint(query_input, sm_client, embedding_model_endpoint)
    source_includes = ["doc_type", "doc"]
    knn_result = search_using_aos_knn(query_embedding[0], aos_endpoint, aos_index, source_includes, aos_result_num)
    result = json.loads(knn_result)
    opensearch_knn_respose = [{"doc": result["hits"]["hits"][i]["_source"]["doc"],
               "doc_type":result["hits"]["hits"][i]["_source"]["doc_type"],
               "score":result["hits"]["hits"][i]["_score"]} for i in range(3)]
    
    # 4. todo: get AOS invertedIndex recall

    # 5. build prompt
    TOKENZIER_MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
    tokenizer = None # AutoTokenizer.from_pretrained(TOKENZIER_MODEL_NAME)

    query_type, prompt_data, log_json = prompt_build(post_text=query_input, opensearch_respose="", opensearch_knn_respose=opensearch_knn_respose,
                                  kendra_respose=kendra_respose, conversations=[], tokenizer=tokenizer)
    
    llm_generation = Generate(sm_client, llm_endpoint, prompt=prompt_data)
    answer = json.loads(llm_generation)['outputs'][len(prompt_data):]

    log_json['session_id'] = session_id
    log_json['chatbot_answer'] = answer
    log_json['conversations'] = conversations
    json_obj_str = json.dumps(json_obj)
    logger.info(json_obj_str)
    
    update_session(session_id=session_id, question=query_input, answer=answer)

    return answer


@handle_error
def lambda_handler(event, context):

    # "model": æ¨¡å‹çš„åç§°
    # "chat_name": å¯¹è¯æ ‡è¯†ï¼Œåç«¯ç”¨æ¥å­˜å‚¨æŸ¥æ‰¾å®ç°å¤šè½®å¯¹è¯ session
    # "prompt": ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    # "max_tokens": 2048
    # "temperature": 0.9
    logger.info(f"event:{event}")
    session_id = event['chat_name']
    question = event['prompt']

    # è·å–å½“å‰æ—¶é—´æˆ³
    request_timestamp = int(time.time())  # æˆ–è€…ä½¿ç”¨ time.time_ns() è·å–çº³ç§’çº§åˆ«çš„æ—¶é—´æˆ³
    logger.info(f'request_timestamp :{request_timestamp}')
    logger.info(f"event:{event}")
    logger.info(f"context:{context}")

    # åˆ›å»ºæ—¥å¿—ç»„å’Œæ—¥å¿—æµ
    log_group_name = '/aws/lambda/{}'.format(context.function_name)
    log_stream_name = context.aws_request_id
    client = boto3.client('logs')
    # æ¥æ”¶è§¦å‘AWS Lambdaå‡½æ•°çš„äº‹ä»¶
    logger.info('The main brain has been activated, awsğŸš€!')

    # 1. è·å–ç¯å¢ƒå˜é‡

    embedding_endpoint = os.environ.get("embedding_endpoint", "")
    aos_endpoint = os.environ.get("aos_endpoint", "")
    aos_index = os.environ.get("aos_index", "")
    aos_knn_field = os.environ.get("aos_knn_field", "")
    aos_result_num = int(os.environ.get("aos_results", ""))

    Kendra_index_id = os.environ.get("Kendra_index_id", "")
    Kendra_result_num = int(os.environ.get("Kendra_result_num", ""))
    # Opensearch_result_num = int(os.environ.get("Opensearch_result_num", ""))

    logger.info(f'embedding_endpoint : {embedding_endpoint}')
    logger.info(f'aos_endpoint : {aos_endpoint}')
    logger.info(f'aos_index : {aos_index}')
    logger.info(f'aos_knn_field : {aos_knn_field}')
    logger.info(f'aos_result_num : {aos_result_num}')
    logger.info(f'Kendra_index_id : {Kendra_index_id}')
    logger.info(f'Kendra_result_num : {Kendra_result_num}')
    
    answer = main_entry(session_id, question, embedding_endpoint, llm_endpoint, aos_endpoint, aos_index, aos_knn_field, aos_result_num,
                       Kendra_index_id, Kendra_result_num)

    # 2. return rusult

    # å¤„ç†

    # Response:
    # "id": "è®¾ç½®ä¸€ä¸ªuuid"
    # "created": "1681891998"
    # "model": "æ¨¡å‹åç§°"
    # "choices": [{"text": "æ¨¡å‹å›ç­”çš„å†…å®¹"}]
    # "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}}]

    return {
        'statusCode': 200,
        'headers': {'Content-Type': 'application/json'},
        'body': [{"id": str(uuid.uuid4()),
                             "created": request_timestamp,
                             "useTime": int(time.time()) - request_timestamp,
                             "model": "main_brain",
                             "choices":
                             [{"text": answer}],
                             "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}},
                            # {"id": uuid.uuid4(),
                            #  "created": request_timestamp,
                            #  "useTime": int(time.time()) - request_timestamp,
                            #  "model": "æ¨¡å‹åç§°",
                            #  "choices":
                            #  [{"text": "2 æ¨¡å‹å›ç­”çš„å†…å®¹"}],
                            #  "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}}
                            ]
    }

