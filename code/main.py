import json
import logging
import os
from botocore import config
from botocore.exceptions import ClientError
from datetime import datetime, timedelta
import boto3
import time
import requests
import uuid
# from transformers import AutoTokenizer
from enum import Enum
from opensearchpy import OpenSearch, RequestsHttpConnection


logger = logging.getLogger()
logger.setLevel(logging.INFO)
sm_client = boto3.client("sagemaker-runtime")
llm_endpoint = 'bloomz-7b1-mt-2023-04-19-09-41-24-189-endpoint'
QA_SEP = "=>"

class ErrorCode:
    DUPLICATED_INDEX_PREFIX = "DuplicatedIndexPrefix"
    DUPLICATED_WITH_INACTIVE_INDEX_PREFIX = "DuplicatedWithInactiveIndexPrefix"
    OVERLAP_INDEX_PREFIX = "OverlapIndexPrefix"
    OVERLAP_WITH_INACTIVE_INDEX_PREFIX = "OverlapWithInactiveIndexPrefix"
    INVALID_INDEX_MAPPING = "InvalidIndexMapping"


class APIException(Exception):
    def __init__(self, message, code: str = None):
        if code:
            super().__init__("[{}] {}".format(code, message))
        else:
            super().__init__(message)


def handle_error(func):
    """Decorator for exception handling"""

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except APIException as e:
            logger.exception(e)
            raise e
        except Exception as e:
            logger.exception(e)
            raise RuntimeError(
                "Unknown exception, please check Lambda log for more details"
            )

    return wrapper

# kendra


def query_kendra(Kendra_index_id="", lang="zh", search_query_text="what is s3?", Kendra_result_num=3):
    # è¿æ¥åˆ°Kendra
    client = boto3.client('kendra')

    # æ„é€ KendraæŸ¥è¯¢è¯·æ±‚
    query_result = client.query(
        IndexId=Kendra_index_id,
        QueryText=search_query_text,
        AttributeFilter={
            "EqualsTo": {
                "Key": "_language_code",
                "Value": {
                    "StringValue": lang
                }
            }
        }
    )
    # print(query_result['ResponseMetadata']['HTTPHeaders'])
    # kendra_took = query_result['ResponseMetadata']['HTTPHeaders']['x-amz-time-millis']
    # åˆ›å»ºä¸€ä¸ªç»“æœåˆ—è¡¨
    results = []

    # å°†æ¯ä¸ªç»“æœæ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­
    for result in query_result['ResultItems']:
        # åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥ä¿å­˜æ¯ä¸ªç»“æœ
        result_dict = {}

        result_dict['score'] = 0.0
        result_dict['doc_type'] = "P"

        # å¦‚æœæœ‰å¯ç”¨çš„æ€»ç»“
        if 'DocumentExcerpt' in result:
            result_dict['doc'] = result['DocumentExcerpt']['Text']
        else:
            result_dict['doc'] = ''

        # å°†ç»“æœæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        results.append(result_dict)

    # è¾“å‡ºç»“æœåˆ—è¡¨
    return results[:Kendra_result_num]


# AOS
def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):
    parameters = {
        # "early_stopping": True,
        # "length_penalty": 2.0,
        "max_new_tokens": 50,
        "temperature": 0,
        "min_length": 10,
        "no_repeat_ngram_size": 2,
    }

    response_model = sm_client.invoke_endpoint(
        EndpointName=endpoint_name,
        Body=json.dumps(
            {
                "inputs": questions,
                "parameters": parameters
            }
        ),
        ContentType="application/json",
    )
    json_str = response_model['Body'].read().decode('utf8')
    json_obj = json.loads(json_str)
    embeddings = json_obj['sentence_embeddings']
    return embeddings

def search_using_aos_knn(q_embedding, hostname, index, size=10):
    # awsauth = (username, passwd)
    # print(type(q_embedding))
    # logger.info(f"q_embedding:")
    # logger.info(q_embedding)
    headers = {"Content-Type": "application/json"}
    # query = {
    #     "size": size,
    #     "query": {
    #         "bool": {
    #             "must":[ {"term": { "doc_type": "P" }} ],
    #             "should": [ { "knn": { "embedding": { "vector": q_embedding, "k": size }}} ]
    #         }
    #     },
    #     "sort": [
    #         {
    #             "_score": {
    #                 "order": "asc"
    #             }
    #         }
    #     ]
    # }

    # reference: https://opensearch.org/docs/latest/search-plugins/knn/filter-search-knn/#boolean-filter-with-ann-search
    # query =  {
    #     "bool": {
    #         "filter": {
    #             "bool": {
    #                 "must": [{ "term": {"doc_type": "P" }}]
    #             }
    #         },
    #         "must": [
    #             {
    #                 "knn": {"embedding": { "vector": q_embedding, "k": size }}
    #             } 
    #         ]
    #     }
    # }


    #Note: æŸ¥è¯¢æ—¶æ— éœ€æŒ‡å®šæ’åºæ–¹å¼ï¼Œæœ€ä¸´è¿‘çš„å‘é‡åˆ†æ•°è¶Šé«˜ï¼Œåšè¿‡å½’ä¸€åŒ–(0.0~1.0)
    query = {
        "size": size,
        "query": {
            "knn": {
                "embedding": {
                    "vector": q_embedding,
                    "k": size
                }
            }
        }
    }
    r = requests.post("https://"+hostname + "/" + index +
                        '/_search', headers=headers, json=query)
    
    results = json.loads(r.text)["hits"]["hits"]
    opensearch_knn_respose = []
    for item in results:
        opensearch_knn_respose.append( {'doc':"{}{}{}".format(item['_source']['doc'], QA_SEP, item['_source']['answer']),"doc_type":item["_source"]["doc_type"],"score":item["_score"]} )
    return opensearch_knn_respose

def aos_search(host, index_name, field, query_term, exactly_match=False, size=10):
    """
    search opensearch with query.
    :param host: AOS endpoint
    :param index_name: Target Index Name
    :param field: search field
    :param query_term: query term
    :return: aos response json
    """
    client = OpenSearch(
        hosts=[{'host': host, 'port': 443}],
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection
    )
    query = None
    if exactly_match:
        query =  {
            "query" : {
                "match_phrase":{
                    "doc": query_term
                }
            }
        }
    else:
        query = {
            "size": size,
            "query": {
                "bool":{
                    "must":[ {"term": { "doc_type": "Q" }} ],
                    "should": [ {"match": { field : query_term }} ]
                }
            },
            "sort": [
                {
                    "_score": {
                        "order": "desc"
                    }
                }
            ]
        }
    query_response = client.search(
        body=query,
        index=index_name
    )

    if exactly_match:
        result_arr = [ {'doc': item['_source']['answer'], 'doc_type': 'A', 'score': item['_score']} for item in query_response["hits"]["hits"]]
    else:
        result_arr = [ {'doc':"{}{}{}".format(item['_source']['doc'], QA_SEP, item['_source']['answer']), 'doc_type': item['_source']['doc_type'], 'score': item['_score']} for item in query_response["hits"]["hits"]]

    return result_arr

def get_session(session_id):

    table_name = "chatbot-session"
    dynamodb = boto3.resource('dynamodb')

    # table name
    table = dynamodb.Table(table_name)
    operation_result = ""

    response = table.get_item(Key={'session-id': session_id})

    if "Item" in response.keys():
        # print("****** " + response["Item"]["content"])
        operation_result = json.loads(response["Item"]["content"])
    else:
        # print("****** No result")
        operation_result = ""

    return operation_result


# param:    session_id
#           question
#           answer
# return:   success
#           failed
def update_session(session_id, question, answer, intention):

    table_name = "chatbot-session"
    dynamodb = boto3.resource('dynamodb')

    # table name
    table = dynamodb.Table(table_name)
    operation_result = ""

    response = table.get_item(Key={'session-id': session_id})

    if "Item" in response.keys():
        # print("****** " + response["Item"]["content"])
        chat_history = json.loads(response["Item"]["content"])
    else:
        # print("****** No result")
        chat_history = []

    chat_history.append([question, answer, intention])
    content = json.dumps(chat_history)

    # inserting values into table
    response = table.put_item(
        Item={
            'session-id': session_id,
            'content': content
        }
    )

    if "ResponseMetadata" in response.keys():
        if response["ResponseMetadata"]["HTTPStatusCode"] == 200:
            operation_result = "success"
        else:
            operation_result = "failed"
    else:
        operation_result = "failed"

    return operation_result


def Generate(smr_client, llm_endpoint, prompt):
    parameters = {
        # "early_stopping": True,
        "length_penalty": 1.0,
        "max_new_tokens": 200,
        "temperature": 0,
        "min_length": 20,
        "no_repeat_ngram_size": 200,
        # "eos_token_id": ['\n']
    }

    response_model = smr_client.invoke_endpoint(
        EndpointName=llm_endpoint,
        Body=json.dumps(
            {
                "inputs": prompt,
                "parameters": parameters
            }
        ),
        ContentType="application/json",
    )
    
    json_ret = json.loads(response_model['Body'].read().decode('utf8'))

    return json_ret['outputs']


class QueryType(Enum):
    KeywordQuery   = "KeywordQuery"       #ç”¨æˆ·ä»…ä»…è¾“å…¥äº†ä¸€äº›å…³é”®è¯ï¼ˆ2 token)
    KnowledgeQuery = "KnowledgeQuery"     #ç”¨æˆ·è¾“å…¥çš„éœ€è¦å‚è€ƒçŸ¥è¯†åº“æœ‰å…³æ¥å›ç­”
    Conversation   = "Conversation"       #ç”¨æˆ·è¾“å…¥çš„æ˜¯è·ŸçŸ¥è¯†åº“æ— å…³çš„é—®é¢˜


def intention_classify(post_text, prompt_template, few_shot_example):
    prompt = prompt_template.format(
        fewshot=few_shot_example, question=post_text)
    result = Generate(sm_client, llm_endpoint, prompt)
    len_prompt = len(prompt)
    return result[len_prompt:]

def intention_detect_prompt_build(post_text, conversations):
    Game_Intention_Classify_Examples="""ç©å®¶è¾“å…¥: ä»‹ç»ä¸€ä¸‹è”ç›Ÿ?
    è¾“å‡º: åŠŸèƒ½ç›¸å…³

    ç©å®¶è¾“å…¥: ä»‹ç»ä¸€ä¸‹å¼ºåŒ–éƒ¨ä»¶?
    è¾“å‡º: åŠŸèƒ½ç›¸å…³

    ç©å®¶è¾“å…¥:æˆ‘ç©ä½ ä»¬çš„æ¸¸æˆå¯¼è‡´å¤±æ‹äº†
    è¾“å‡º: åŠŸèƒ½æ— å…³

    ç©å®¶è¾“å…¥: æˆ‘è¦ç»™ä½ ä»¬æä¸ªå»ºè®®ï¼
    è¾“å‡º: åŠŸèƒ½æ— å…³

    ç©å®¶è¾“å…¥: ä»Šå¤©å¿ƒæƒ…ä¸å¥½
    è¾“å‡º: åŠŸèƒ½æ— å…³

    ç©å®¶è¾“å…¥: å’Œå¥³æœ‹å‹åµæ¶äº†
    è¾“å‡º: åŠŸèƒ½æ— å…³

    ç©å®¶è¾“å…¥: æˆ‘çœŸçš„æ— è¯­äº†
    è¾“å‡º: åŠŸèƒ½æ— å…³

    ç©å®¶è¾“å…¥: å¿ƒæƒ…çœŸçš„å¾ˆå·®ï¼Œæ€ä¹ˆåŠ
    è¾“å‡º: åŠŸèƒ½æ— å…³

    ç©å®¶è¾“å…¥:æ€ä¹ˆæ‰èƒ½è¿ç§»åŒºæœï¼Ÿ
    è¾“å‡º: åŠŸèƒ½ç›¸å…³"""

    Game_Intention_Classify_Prompt = """
    ä»»åŠ¡å®šä¹‰: åˆ¤æ–­ç©å®¶è¾“å…¥æ˜¯å¦åœ¨è¯¢é—®æ¸¸æˆåŠŸèƒ½ç›¸å…³çš„é—®é¢˜, å›ç­” "åŠŸèƒ½ç›¸å…³" æˆ–è€… "åŠŸèƒ½æ— å…³".

    {fewshot}

    ç©å®¶è¾“å…¥:{question}
    è¾“å‡º: """
    pass

def conversion_prompt_build(post_text, conversations, role_a="ç©å®¶", role_b = "Jarvis"):
    Game_Free_Chat_Examples ="""{A}: æˆ‘è¦ç»™ä½ ä»¬æä¸ªå»ºè®®ï¼
    {B}: æ„Ÿè°¢æ‚¨çš„æ”¯æŒï¼Œæˆ‘ä»¬å¯¹ç©å®¶çš„å»ºè®®éƒ½æ˜¯éå¸¸é‡è§†çš„ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»å³ä¸Šè§’è”ç³»å®¢æœ-æˆ‘è¦æäº¤å»ºè®®å¡«å†™æ‚¨çš„å»ºè®®å†…å®¹å¹¶æäº¤ï¼Œæˆ‘ä»¬ä¼šè½¬è¾¾ç»™å›¢é˜Ÿè¿›è¡Œè€ƒé‡ã€‚å»ºè®®æäº¤

    {A}: æˆ‘ç©ä½ ä»¬çš„æ¸¸æˆå¯¼è‡´å¤±æ‹äº†
    {B}: äº²çˆ±çš„ç©å®¶ï¼ŒçœŸçš„éå¸¸æŠ±æ­‰å¬åˆ°è¿™ä¸ªæ¶ˆæ¯ï¼Œè®©æ‚¨å—åˆ°äº†å›°æ‰°ã€‚æ„Ÿæƒ…çš„äº‹æƒ…ç¡®å®å¾ˆå¤æ‚ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œæ—¶é—´ä¼šæ²»æ„ˆä¸€åˆ‡ã€‚è¯·ä¿æŒä¹è§‚ç§¯æçš„å¿ƒæ€ï¼Œä¹Ÿè®¸æœªæ¥ä¼šæœ‰æ›´å¥½çš„äººé™ªä¼´æ‚¨ã€‚æˆ‘ä»¬ä¼šä¸€ç›´é™ªåœ¨æ‚¨èº«è¾¹ï¼Œä¸ºæ‚¨æä¾›æ¸¸æˆä¸­çš„æ”¯æŒä¸å¸®åŠ©ã€‚è¯·å¤šå…³æ³¨è‡ªå·±çš„ç”Ÿæ´»ï¼Œé€‚å½“è°ƒæ•´æ¸¸æˆä¸ç”Ÿæ´»çš„å¹³è¡¡ã€‚ç¥æ‚¨ç”Ÿæ´»æ„‰å¿«ï¼Œæ„Ÿæƒ…ç¾æ»¡~(ã¥ï½¡â—•â€¿â€¿â—•ï½¡)ã¥
    """.format(A=role_a, B = role_b)

    chat_history = [ """{}: {}\n{}: {}""".format(role_a, item[0], role_b, item[1]) for item in conversations ]
    chat_histories = "\n\n".join(chat_history)

    Game_Free_Chat_Prompt = """
    Jarvis æ˜¯ä¸€ä¸ªæ¸¸æˆæ™ºèƒ½å®¢æœï¼Œèƒ½å¤Ÿå›ç­”ç©å®¶çš„å„ç§é—®é¢˜ä»¥åŠé™ªç”¨æˆ·èŠå¤©ï¼Œæ¯”å¦‚\n\n{fewshot}\n\n{chat_history}\n\n{A}: {question}\n{B}: """
    return Game_Free_Chat_Prompt.format(fewshot=Game_Free_Chat_Examples, chat_history=chat_histories, question=post_text, A=role_a, B=role_b)

# different scan
def qa_knowledge_prompt_build(post_text, qa_recalls, role_a="ç©å®¶", role_b = "Jarvis"):
    """
    Detect User intentions, build prompt for LLM. For Knowledge QA, it will merge all retrieved related document paragraphs into a single prompt
    Parameters examples:
        post_text : "ä»‹ç»ä¸‹å¼ºåŒ–éƒ¨ä»¶"
        qa_recalls: [ doc1, doc2, ]
    return: prompt string
    """
    qa_pairs = [ doc.split(QA_SEP) for doc, _ in qa_recalls ]
    qa_fewshots = [ "{}: {}\n{}: {}".format(role_a, pair[0], role_b, pair[1]) for pair in qa_pairs ]
    fewshots_str = "\n\n".join(qa_fewshots[-3:])
    Game_Knowledge_QA_Prompt = """{AI_role} æ˜¯ã€Šå£è¢‹å¥‡å…µã€‹æ¸¸æˆçš„æ™ºèƒ½å®¢æœï¼Œèƒ½å¤Ÿå›ç­”ç©å®¶çš„å„ç§é—®é¢˜ï¼Œæ¯”å¦‚\n\n{fewshot}\n\nç©å®¶:{question}\nJarvis:"""

    return Game_Knowledge_QA_Prompt.format(fewshot=fewshots_str, question=post_text, AI_role=role_b)

def main_entry(session_id:str, query_input:str, embedding_model_endpoint:str, llm_model_endpoint:str, aos_endpoint:str, aos_index:str, aos_knn_field:str, aos_result_num:int, kendra_index_id:str, kendra_result_num:int):
    """
    Entry point for the Lambda function.

    Parameters:
        session_id (str): The ID of the session.
        query_input (str): The query input.
        embedding_model_endpoint (str): The endpoint of the embedding model.
        llm_model_endpoint (str): The endpoint of the language model.
        aos_endpoint (str): The endpoint of the AOS engine.
        aos_index (str): The index of the AOS engine.
        aos_knn_field (str): The knn field of the AOS engine.
        aos_result_num (int): The number of results of the AOS engine.
        kendra_index_id (str): The ID of the Kendra index.
        kendra_result_num (int): The number of results of the Kendra Service.

    return: answer(str)
    """
    sm_client = boto3.client("sagemaker-runtime")
    
    # 1. get_session
    import time
    start1 = time.time()
    session_history = get_session(session_id=session_id)
    elpase_time = time.time() - start1
    logger.info(f'runing time of get_session : {elpase_time}s seconds')

    # 2. get kendra recall 
    # start = time.time()
    # kendra_respose = [] # query_kendra(kendra_index_id, "zh", query_input, kendra_result_num)
    # elpase_time = time.time() - start
    # logger.info(f'runing time of query_kendra : {elpase_time}s seconds')

    # 3. get AOS knn recall 
    start = time.time()
    query_embedding = get_vector_by_sm_endpoint(query_input, sm_client, embedding_model_endpoint)
    opensearch_knn_respose = search_using_aos_knn(query_embedding[0], aos_endpoint, aos_index)
    elpase_time = time.time() - start
    logger.info(f'runing time of opensearch_knn : {elpase_time}s seconds')
    
    # 4. get AOS invertedIndex recall
    start = time.time()
    opensearch_query_response = aos_search(aos_endpoint, aos_index, "doc", query_input)
    # logger.info(opensearch_query_response)
    elpase_time = time.time() - start
    logger.info(f'runing time of opensearch_query : {elpase_time}s seconds')

    # 5. combine these two opensearch_knn_respose and opensearch_query_response
    def combine_recalls(opensearch_knn_respose, opensearch_query_response):
        '''
        filter knn_result if the result don't appear in filter_inverted_result
        '''
        knn_threshold = 0.3
        inverted_theshold = 5.0
        filter_knn_result = { item["doc"] : item["score"] for item in opensearch_knn_respose if item["score"]> knn_threshold }
        filter_inverted_result = { item["doc"] : item["score"] for item in opensearch_query_response if item["score"]> inverted_theshold }

        combine_result = []
        for doc, score in filter_knn_result.items():
            if doc in filter_inverted_result.keys():
                combine_result.append(( doc, score))

        return combine_result
    
    recall_knowledge = combine_recalls(opensearch_knn_respose, opensearch_query_response)

    # 6. check is it keyword search
    exactly_match_result = aos_search(aos_endpoint, aos_index, "doc", query_input, exactly_match=True)

    answer = None
    final_prompt = None
    query_type = None
    if exactly_match_result and recall_knowledge: 
        query_type = QueryType.KeywordQuery
        answer = exactly_match_result[0]["doc"]
        final_prompt = ""
    elif recall_knowledge:
        query_type = QueryType.KnowledgeQuery
        final_prompt = qa_knowledge_prompt_build(query_input, recall_knowledge, role_a="ç©å®¶", role_b = "Jarvis")
    else:
        query_type = QueryType.Conversation
        free_chat_coversions = [ item for item in session_history if item[2] == "QueryType.Conversation" ]
        final_prompt = conversion_prompt_build(query_input, free_chat_coversions)

    json_obj = {
        "query": query_input,
        "opensearch_doc":  opensearch_query_response,
        "opensearch_knn_doc":  opensearch_knn_respose,
        "kendra_doc": [],
        "knowledges" : recall_knowledge,
        "detect_query_type": str(query_type),
        "LLM_input": final_prompt
    }

    try:
        llm_generation = Generate(sm_client, llm_endpoint, prompt=final_prompt)
        answer = llm_generation[len(final_prompt):]
        json_obj['session_id'] = session_id
        json_obj['chatbot_answer'] = answer
        json_obj['conversations'] = free_chat_coversions
        json_obj['log_type'] = "all"
        json_obj_str = json.dumps(json_obj, ensure_ascii=False)
    except Exception as e:
        logger.info(f'Exceptions: str({e})')
    finally:
        json_obj_str = json.dumps(json_obj, ensure_ascii=False)
        logger.info(json_obj_str)

    start = time.time()
    update_session(session_id=session_id, question=query_input, answer=answer, intention=str(query_type))
    elpase_time = time.time() - start
    elpase_time1 = time.time() - start1
    logger.info(f'runing time of update_session : {elpase_time}s seconds')
    logger.info(f'runing time of all  : {elpase_time1}s seconds')

    return answer


@handle_error
def lambda_handler(event, context):

    # "model": æ¨¡å‹çš„åç§°
    # "chat_name": å¯¹è¯æ ‡è¯†ï¼Œåç«¯ç”¨æ¥å­˜å‚¨æŸ¥æ‰¾å®ç°å¤šè½®å¯¹è¯ session
    # "prompt": ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    # "max_tokens": 2048
    # "temperature": 0.9
    logger.info(f"event:{event}")
    session_id = event['chat_name']
    question = event['prompt']

    # è·å–å½“å‰æ—¶é—´æˆ³
    request_timestamp = time.time()  # æˆ–è€…ä½¿ç”¨ time.time_ns() è·å–çº³ç§’çº§åˆ«çš„æ—¶é—´æˆ³
    logger.info(f'request_timestamp :{request_timestamp}')
    logger.info(f"event:{event}")
    logger.info(f"context:{context}")

    # åˆ›å»ºæ—¥å¿—ç»„å’Œæ—¥å¿—æµ
    log_group_name = '/aws/lambda/{}'.format(context.function_name)
    log_stream_name = context.aws_request_id
    client = boto3.client('logs')
    # æ¥æ”¶è§¦å‘AWS Lambdaå‡½æ•°çš„äº‹ä»¶
    logger.info('The main brain has been activated, awsğŸš€!')

    # 1. è·å–ç¯å¢ƒå˜é‡

    embedding_endpoint = os.environ.get("embedding_endpoint", "")
    aos_endpoint = os.environ.get("aos_endpoint", "")
    aos_index = os.environ.get("aos_index", "")
    aos_knn_field = os.environ.get("aos_knn_field", "")
    aos_result_num = int(os.environ.get("aos_results", ""))

    Kendra_index_id = os.environ.get("Kendra_index_id", "")
    Kendra_result_num = int(os.environ.get("Kendra_result_num", ""))
    # Opensearch_result_num = int(os.environ.get("Opensearch_result_num", ""))

    logger.info(f'embedding_endpoint : {embedding_endpoint}')
    logger.info(f'aos_endpoint : {aos_endpoint}')
    logger.info(f'aos_index : {aos_index}')
    logger.info(f'aos_knn_field : {aos_knn_field}')
    logger.info(f'aos_result_num : {aos_result_num}')
    logger.info(f'Kendra_index_id : {Kendra_index_id}')
    logger.info(f'Kendra_result_num : {Kendra_result_num}')
    
    main_entry_start = time.time()  # æˆ–è€…ä½¿ç”¨ time.time_ns() è·å–çº³ç§’çº§åˆ«çš„æ—¶é—´æˆ³
    answer = main_entry(session_id, question, embedding_endpoint, llm_endpoint, aos_endpoint, aos_index, aos_knn_field, aos_result_num,
                       Kendra_index_id, Kendra_result_num)
    main_entry_elpase = time.time() - main_entry_start  # æˆ–è€…ä½¿ç”¨ time.time_ns() è·å–çº³ç§’çº§åˆ«çš„æ—¶é—´æˆ³
    logger.info(f'runing time of main_entry : {main_entry_elpase}s seconds')
    # 2. return rusult

    # å¤„ç†

    # Response:
    # "id": "è®¾ç½®ä¸€ä¸ªuuid"
    # "created": "1681891998"
    # "model": "æ¨¡å‹åç§°"
    # "choices": [{"text": "æ¨¡å‹å›ç­”çš„å†…å®¹"}]
    # "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}}]

    return {
        'statusCode': 200,
        'headers': {'Content-Type': 'application/json'},
        'body': [{"id": str(uuid.uuid4()),
                             "created": request_timestamp,
                             "useTime": time.time() - request_timestamp,
                             "model": "main_brain",
                             "choices":
                             [{"text": answer}],
                             "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}},
                            # {"id": uuid.uuid4(),
                            #  "created": request_timestamp,
                            #  "useTime": int(time.time()) - request_timestamp,
                            #  "model": "æ¨¡å‹åç§°",
                            #  "choices":
                            #  [{"text": "2 æ¨¡å‹å›ç­”çš„å†…å®¹"}],
                            #  "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}}
                            ]
    }

