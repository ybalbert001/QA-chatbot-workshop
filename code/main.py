import json
import logging
import os
from botocore import config
from botocore.exceptions import ClientError
from datetime import datetime, timedelta
import boto3
import time
import requests
import uuid
# from transformers import AutoTokenizer
from enum import Enum
from opensearchpy import OpenSearch, RequestsHttpConnection


logger = logging.getLogger()
logger.setLevel(logging.INFO)
sm_client = boto3.client("sagemaker-runtime")
llm_endpoint = 'bloomz-7b1-mt-2023-04-19-09-41-24-189-endpoint'

class ErrorCode:
    DUPLICATED_INDEX_PREFIX = "DuplicatedIndexPrefix"
    DUPLICATED_WITH_INACTIVE_INDEX_PREFIX = "DuplicatedWithInactiveIndexPrefix"
    OVERLAP_INDEX_PREFIX = "OverlapIndexPrefix"
    OVERLAP_WITH_INACTIVE_INDEX_PREFIX = "OverlapWithInactiveIndexPrefix"
    INVALID_INDEX_MAPPING = "InvalidIndexMapping"


class APIException(Exception):
    def __init__(self, message, code: str = None):
        if code:
            super().__init__("[{}] {}".format(code, message))
        else:
            super().__init__(message)


def handle_error(func):
    """Decorator for exception handling"""

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except APIException as e:
            logger.exception(e)
            raise e
        except Exception as e:
            logger.exception(e)
            raise RuntimeError(
                "Unknown exception, please check Lambda log for more details"
            )

    return wrapper

# kendra


def query_kendra(Kendra_index_id="", lang="zh", search_query_text="what is s3?", Kendra_result_num=3):
    # è¿æ¥åˆ°Kendra
    client = boto3.client('kendra')

    # æ„é€ KendraæŸ¥è¯¢è¯·æ±‚
    query_result = client.query(
        IndexId=Kendra_index_id,
        QueryText=search_query_text,
        AttributeFilter={
            "EqualsTo": {
                "Key": "_language_code",
                "Value": {
                    "StringValue": lang
                }
            }
        }
    )
    # print(query_result['ResponseMetadata']['HTTPHeaders'])
    # kendra_took = query_result['ResponseMetadata']['HTTPHeaders']['x-amz-time-millis']
    # åˆ›å»ºä¸€ä¸ªç»“æœåˆ—è¡¨
    results = []

    # å°†æ¯ä¸ªç»“æœæ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­
    for result in query_result['ResultItems']:
        # åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥ä¿å­˜æ¯ä¸ªç»“æœ
        result_dict = {}

        result_dict['score'] = 0.0
        result_dict['doc_type'] = "P"

        # å¦‚æœæœ‰å¯ç”¨çš„æ€»ç»“
        if 'DocumentExcerpt' in result:
            result_dict['doc'] = result['DocumentExcerpt']['Text']
        else:
            result_dict['doc'] = ''

        # å°†ç»“æœæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        results.append(result_dict)

    # è¾“å‡ºç»“æœåˆ—è¡¨
    return results[:Kendra_result_num]


# AOS
def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):
    parameters = {
        # "early_stopping": True,
        # "length_penalty": 2.0,
        "max_new_tokens": 50,
        "temperature": 0,
        "min_length": 10,
        "no_repeat_ngram_size": 2,
    }

    response_model = sm_client.invoke_endpoint(
        EndpointName=endpoint_name,
        Body=json.dumps(
            {
                "inputs": questions,
                "parameters": parameters
            }
        ),
        ContentType="application/json",
    )
    json_str = response_model['Body'].read().decode('utf8')
    json_obj = json.loads(json_str)
    embeddings = json_obj['sentence_embeddings']
    return embeddings

def search_using_aos_knn(q_embedding, hostname, index, source_includes, size):
    # awsauth = (username, passwd)
    # print(type(q_embedding))
    headers = {"Content-Type": "application/json"}
    query = {
        "size": size,
        "query": {
            "knn": {
                "embedding": {
                    "vector": q_embedding,
                    "k": size
                }
            }
        }
    }
    r = requests.post("https://"+hostname + "/" + index +
                        '/_search', headers=headers, json=query)
    return r.text

def aos_search(host, index_name, field, query_term):
    """
    search opensearch with query.
    :param host: AOS endpoint
    :param index_name: Target Index Name
    :param field: search field
    :param query_term: query term
    :return: aos response json
    """
    client = OpenSearch(
        hosts=[{'host': host, 'port': 443}],
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection
    )
    query = {
        "query": {
            'match': {
                field: query_term
            }
        }
    }
    query_response = client.search(
        body=query,
        index=index_name
    )

    result_arr = [ {'doc':item['_source']['doc'], 'doc_type': item['_source']['doc_type'], 'score': item['_score']} for item in query_response["hits"]["hits"]]

    return result_arr
# def query_opensearch(api_url, query_type, query_params):
#     headers = {'Content-Type': 'application/json'}
#     if query_type == 'vector':  # å‘é‡æŸ¥è¯¢
#         query_json = {
#             "size": query_params['size'],
#             "query": {
#                 "knn": {
#                     query_params['field']: {
#                         "vector": query_params['vector_values'],
#                         "k": query_params['k']
#                     }
#                 }
#             }
#         }
#     elif query_type == 'match': # å­—ç¬¦ä¸²åŒ¹é…æŸ¥è¯¢
#         query_json = {
#             "size": query_params['size'],
#             "query": {
#                 "match": {
#                     query_params['field']: query_params['match_keyword']
#                 }
#             }
#         }

#     response = requests.post(api_url, headers=headers, data=json.dumps(query_json))
#     if response.status_code == 200:
#         # è¯·æ±‚æˆåŠŸ
#         response_json = json.loads(response.text)
#         hits = response_json['hits']['hits']
#         return [hit['_source'] for hit in hits]
#     else:
#         # è¯·æ±‚å¤±è´¥
#         print(f"è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
#         return []


# def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name, parameters):
#     response_model = sm_client.invoke_endpoint(
#         EndpointName=endpoint_name,
#         Body=json.dumps(
#             {
#                 "inputs": questions,
#                 "parameters": parameters
#             }
#         ),
#         ContentType="application/json",
#     )
#     json_str = response_model['Body'].read().decode('utf8')
#     json_obj = json.loads(json_str)
#     embeddings = json_obj['sentence_embeddings']
#     return embeddings


# def search_using_aos_knn(q_embedding, hostname, index, source_includes, size):
#     # awsauth = (username, passwd)
#     print(type(q_embedding))
#     query = {
#         "size": size,
#         "query": {
#             "knn": {
#                 "sentence_vector": {
#                     "vector": q_embedding,
#                     "k": size
#                 }
#             }
#         }
#     }
#     r = requests.post("https://"+hostname + "/" + index +
#                       '/_search', headers=headers, json=query)
#     return r.text

# DDB


def get_session(session_id):

    table_name = "chatbot-session"
    dynamodb = boto3.resource('dynamodb')

    # table name
    table = dynamodb.Table(table_name)
    operation_result = ""

    response = table.get_item(Key={'session-id': session_id})

    if "Item" in response.keys():
        print("****** " + response["Item"]["content"])
        operation_result = json.loads(response["Item"]["content"])
    else:
        print("****** No result")
        operation_result = ""

    return operation_result


# param:    session_id
#           question
#           answer
# return:   success
#           failed
def update_session(session_id, question, answer, intention):

    table_name = "chatbot-session"
    dynamodb = boto3.resource('dynamodb')

    # table name
    table = dynamodb.Table(table_name)
    operation_result = ""

    response = table.get_item(Key={'session-id': session_id})

    if "Item" in response.keys():
        print("****** " + response["Item"]["content"])
        chat_history = json.loads(response["Item"]["content"])
    else:
        print("****** No result")
        chat_history = []

    chat_history.append([question, answer, intention])
    content = json.dumps(chat_history)

    # inserting values into table
    response = table.put_item(
        Item={
            'session-id': session_id,
            'content': content
        }
    )

    if "ResponseMetadata" in response.keys():
        if response["ResponseMetadata"]["HTTPStatusCode"] == 200:
            operation_result = "success"
        else:
            operation_result = "failed"
    else:
        operation_result = "failed"

    return operation_result





def Generate(smr_client, llm_endpoint, prompt):
    parameters = {
        # "early_stopping": True,
        "length_penalty": 100.0,
        "max_new_tokens": 200,
        "temperature": 0,
        "min_length": 20,
        "no_repeat_ngram_size": 200
    }

    response_model = smr_client.invoke_endpoint(
        EndpointName=llm_endpoint,
        Body=json.dumps(
            {
                "inputs": prompt,
                "parameters": parameters
            }
        ),
        ContentType="application/json",
    )
    
    json_ret = json.loads(response_model['Body'].read().decode('utf8'))

    return json_ret['outputs']


class QueryType(Enum):
    KeywordQuery   = "KeywordQuery"       #ç”¨æˆ·ä»…ä»…è¾“å…¥äº†ä¸€äº›å…³é”®è¯ï¼ˆ2 token)
    KnowledgeQuery = "KnowledgeQuery"     #ç”¨æˆ·è¾“å…¥çš„éœ€è¦å‚è€ƒçŸ¥è¯†åº“æœ‰å…³æ¥å›ç­”
    Conversation   = "Conversation"       #ç”¨æˆ·è¾“å…¥çš„æ˜¯è·ŸçŸ¥è¯†åº“æ— å…³çš„é—®é¢˜


def intention_classify(post_text, prompt_template, few_shot_example):
    prompt = prompt_template.format(
        fewshot=few_shot_example, question=post_text)
    print(prompt)
    result = Generate(sm_client, llm_endpoint, prompt)
    print(result)
    len_prompt = len(prompt)
    return result[len_prompt:]

# different scan
def prompt_build(post_text, opensearch_respose, opensearch_knn_respose, kendra_respose, conversations, tokenizer):
    """
    Detect User intentions, build prompt for LLM. For Knowledge QA, it will merge all retrieved related document paragraphs into a single prompt
    Parameters examples:
        post_text : "ä½ å¥½ä¹ˆï¼Ÿ"
        opensearch_respose: [{"score" : 0.7, "doc": "....", "doc_type": "Q|A|P"}]
        opensearch_knn_respose: [
            {"score" : 0.7, "doc": "....", "doc_type": "Q|A|P"}
        ]
        kendra_respose: [{"score" : 0.7, "doc": "....", "doc_type": "Q|A|P"}]
        conversations: [["Q1", "A1",'Qtype1'], ("Q1", "A1",'Qtype1'), ...]
        tokenizer: which aim to calculate length of query's token
    return: prompt string
    """
    # prompt templates:
    # LLM
    Game_Intention_Classify_Examples="""ç©å®¶è¾“å…¥: ä»‹ç»ä¸€ä¸‹è”ç›Ÿ?
    è¾“å‡º: åŠŸèƒ½ç›¸å…³

    ç©å®¶è¾“å…¥: ä»‹ç»ä¸€ä¸‹å¼ºåŒ–éƒ¨ä»¶?
    è¾“å‡º: åŠŸèƒ½ç›¸å…³

    ç©å®¶è¾“å…¥:æˆ‘ç©ä½ ä»¬çš„æ¸¸æˆå¯¼è‡´å¤±æ‹äº†
    è¾“å‡º: åŠŸèƒ½æ— å…³

    ç©å®¶è¾“å…¥: æˆ‘è¦ç»™ä½ ä»¬æä¸ªå»ºè®®ï¼
    è¾“å‡º: åŠŸèƒ½æ— å…³

    ç©å®¶è¾“å…¥:æ€ä¹ˆæ‰èƒ½è¿ç§»åŒºæœï¼Ÿ
    è¾“å‡º: åŠŸèƒ½ç›¸å…³"""

    Game_Intention_Classify_Prompt = """
    ä»»åŠ¡å®šä¹‰: åˆ¤æ–­ç©å®¶è¾“å…¥æ˜¯å¦åœ¨è¯¢é—®æ¸¸æˆåŠŸèƒ½ç›¸å…³çš„é—®é¢˜, å›ç­” "åŠŸèƒ½ç›¸å…³" æˆ–è€… "åŠŸèƒ½æ— å…³".

    {fewshot}

    ç©å®¶è¾“å…¥:{question}
    è¾“å‡º: """

    Game_Knowledge_QA_Prompt = """
    Jarvis æ˜¯ä¸€ä¸ªæ¸¸æˆæ™ºèƒ½å®¢æœï¼Œèƒ½å¤Ÿå›ç­”ç©å®¶çš„å„ç§é—®é¢˜ï¼Œä»¥åŠé™ªç”¨æˆ·èŠå¤©ï¼Œæ¯”å¦‚

    {fewshot}

    ç©å®¶:{question}
    Jarvis:"""

    Game_Free_Chat_Examples ="""ç©å®¶: æˆ‘è¦ç»™ä½ ä»¬æä¸ªå»ºè®®ï¼
    Jarvis: æ„Ÿè°¢æ‚¨çš„æ”¯æŒï¼Œæˆ‘ä»¬å¯¹ç©å®¶çš„å»ºè®®éƒ½æ˜¯éå¸¸é‡è§†çš„ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»å³ä¸Šè§’è”ç³»å®¢æœ-æˆ‘è¦æäº¤å»ºè®®å¡«å†™æ‚¨çš„å»ºè®®å†…å®¹å¹¶æäº¤ï¼Œæˆ‘ä»¬ä¼šè½¬è¾¾ç»™å›¢é˜Ÿè¿›è¡Œè€ƒé‡ã€‚å»ºè®®æäº¤

    ç©å®¶: æˆ‘ç©ä½ ä»¬çš„æ¸¸æˆå¯¼è‡´å¤±æ‹äº†
    Jarvis:äº²çˆ±çš„ç©å®¶ï¼ŒçœŸçš„éå¸¸æŠ±æ­‰å¬åˆ°è¿™ä¸ªæ¶ˆæ¯ï¼Œè®©æ‚¨å—åˆ°äº†å›°æ‰°ã€‚æ„Ÿæƒ…çš„äº‹æƒ…ç¡®å®å¾ˆå¤æ‚ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œæ—¶é—´ä¼šæ²»æ„ˆä¸€åˆ‡ã€‚è¯·ä¿æŒä¹è§‚ç§¯æçš„å¿ƒæ€ï¼Œä¹Ÿè®¸æœªæ¥ä¼šæœ‰æ›´å¥½çš„äººé™ªä¼´æ‚¨ã€‚æˆ‘ä»¬ä¼šä¸€ç›´é™ªåœ¨æ‚¨èº«è¾¹ï¼Œä¸ºæ‚¨æä¾›æ¸¸æˆä¸­çš„æ”¯æŒä¸å¸®åŠ©ã€‚è¯·å¤šå…³æ³¨è‡ªå·±çš„ç”Ÿæ´»ï¼Œé€‚å½“è°ƒæ•´æ¸¸æˆä¸ç”Ÿæ´»çš„å¹³è¡¡ã€‚ç¥æ‚¨ç”Ÿæ´»æ„‰å¿«ï¼Œæ„Ÿæƒ…ç¾æ»¡~(ã¥ï½¡â—•â€¿â€¿â—•ï½¡)ã¥
    """

    Game_Free_Chat_Prompt = """
    Jarvis æ˜¯ä¸€ä¸ªæ¸¸æˆæ™ºèƒ½å®¢æœï¼Œèƒ½å¤Ÿå›ç­”ç©å®¶çš„å„ç§é—®é¢˜ï¼Œä»¥åŠé™ªç”¨æˆ·èŠå¤©ï¼Œæ¯”å¦‚

    {fewshot}

    {chat_history}

    ç©å®¶: {question}
    Jarvis:"""

    q_type = None
    final_prompt = ""

    # tokens = tokenizer.encode(post_text)

    # if user input exactly match special term, return the special term's description directly
    if (post_text in ["å¼ºåŒ–å¤‡ä»¶", "è”ç›Ÿ"]): # reimplement this judge later
        q_type = QueryType.KeywordQuery
        if opensearch_respose:
            final_prompt = opensearch_respose[-1]
    else:  # NormalQuery
        detect_intention = intention_classify(post_text, Game_Intention_Classify_Prompt, Game_Intention_Classify_Examples)
        print(f"detect_intention : {detect_intention}")
        if detect_intention == "åŠŸèƒ½æ— å…³":
            chat_histories = '\n\n'.join(['\n'.join(conversation[:2]) for conversation in conversations])
            final_prompt = Game_Free_Chat_Prompt.format(fewshot=Game_Free_Chat_Examples, chat_history=chat_histories, question=post_text)
            q_type = QueryType.Conversation
        else:  # detect_intention == "åŠŸèƒ½ç›¸å…³"
            # Combine opensearch_knn_respose and kendra_respose
            recall_dict = {}
            # for recall_item in opensearch_respose:
            #     if recall_item["doc"] in recall_item.keys():
            #         if recall_item["score"] > recall_dict[recall_item["doc"]]:
            #             recall_dict[recall_item["doc"]] = recall_item["score"]
            #     else:
            #         recall_dict[recall_item["doc"]] = recall_item["score"]

            for recall_item in opensearch_knn_respose:
                if recall_item["doc_type"] != 'P':
                    continue

                if recall_item["doc"] in recall_item.keys():
                    if recall_item["score"] > recall_dict[recall_item["doc"]]:
                        recall_dict[recall_item["doc"]] = recall_item["score"]
                else:
                    recall_dict[recall_item["doc"]] = recall_item["score"]

            for recall_item in kendra_respose:
                if recall_item["doc"] in recall_item.keys():
                    if recall_item["score"] > recall_dict[recall_item["doc"]]:
                        recall_dict[recall_item["doc"]] = recall_item["score"]
                else:
                    recall_dict[recall_item["doc"]] = recall_item["score"]

            example_list = [k for k, v in sorted(
                recall_dict.items(), key=lambda item: item[1])]
            q_type = QueryType.KnowledgeQuery
            prompt_context = '\n\n'.join(example_list)

            final_prompt = Game_Knowledge_QA_Prompt.format(fewshot=prompt_context, question=post_text)
            final_prompt = final_prompt.replace("Question", "ç©å®¶").replace("Answer", "Jarvis")

    json_obj = {
        "query": post_text,
        "opensearch_doc":  opensearch_respose,
        "opensearch_knn_doc":  opensearch_knn_respose,
        "kendra_doc": kendra_respose,
        "detect_query_type": str(q_type),
        "LLM_input": final_prompt
    }

    return q_type, final_prompt, json_obj

def main_entry(session_id:str, query_input:str, embedding_model_endpoint:str, llm_model_endpoint:str, aos_endpoint:str, aos_index:str, aos_knn_field:str, aos_result_num:int, kendra_index_id:str, kendra_result_num:int):
    """
    Entry point for the Lambda function.

    Parameters:
        session_id (str): The ID of the session.
        query_input (str): The query input.
        embedding_model_endpoint (str): The endpoint of the embedding model.
        llm_model_endpoint (str): The endpoint of the language model.
        aos_endpoint (str): The endpoint of the AOS engine.
        aos_index (str): The index of the AOS engine.
        aos_knn_field (str): The knn field of the AOS engine.
        aos_result_num (int): The number of results of the AOS engine.
        kendra_index_id (str): The ID of the Kendra index.
        kendra_result_num (int): The number of results of the Kendra Service.

    return: answer(str)
    """
    sm_client = boto3.client("sagemaker-runtime")
    
    # 1. get_session
    session_history = get_session(session_id=session_id)

    # 2. get kendra recall 
    kendra_respose = query_kendra(kendra_index_id, "zh", query_input, kendra_result_num)

    # 3. get AOS knn recall 
    query_embedding = get_vector_by_sm_endpoint(query_input, sm_client, embedding_model_endpoint)
    source_includes = ["doc_type", "doc"]
    knn_result = search_using_aos_knn(query_embedding[0], aos_endpoint, aos_index, source_includes, aos_result_num)
    result = json.loads(knn_result)["hits"]["hits"]
    opensearch_knn_respose = []
    for item in result[:3]:
        opensearch_knn_respose.append( {"doc": item["_source"]["doc"],"doc_type":item["_source"]["doc_type"],"score":item["_score"]} )
    
    # 4. todo: get AOS invertedIndex recall
    opensearch_query_response = aos_search(aos_endpoint, aos_index, "doc", query_input)
    logger.info(opensearch_query_response)

    # 5. build prompt
    TOKENZIER_MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
    tokenizer = None # AutoTokenizer.from_pretrained(TOKENZIER_MODEL_NAME)

    query_type, prompt_data, json_obj = prompt_build(post_text=query_input, opensearch_respose=opensearch_query_response, opensearch_knn_respose=opensearch_knn_respose,
                                  kendra_respose=kendra_respose, conversations=session_history, tokenizer=tokenizer)
    
    answer = None
    try:
        llm_generation = Generate(sm_client, llm_endpoint, prompt=prompt_data)
        answer = llm_generation[len(prompt_data):]
        json_obj['session_id'] = session_id
        json_obj['chatbot_answer'] = answer
        json_obj['conversations'] = session_history
        json_obj['log_type'] = "all"
        json_obj_str = json.dumps(json_obj, ensure_ascii=False)
        logger.info(json_obj_str)
    except Exception as e:
        logger.info(f'Exceptions: str({e})')
    finally:
        json_obj_str = json.dumps(json_obj, ensure_ascii=False)
        logger.info(json_obj_str)

    update_session(session_id=session_id, question=query_input, answer=answer, intention=str(query_type))

    return answer


@handle_error
def lambda_handler(event, context):

    # "model": æ¨¡å‹çš„åç§°
    # "chat_name": å¯¹è¯æ ‡è¯†ï¼Œåç«¯ç”¨æ¥å­˜å‚¨æŸ¥æ‰¾å®ç°å¤šè½®å¯¹è¯ session
    # "prompt": ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    # "max_tokens": 2048
    # "temperature": 0.9
    logger.info(f"event:{event}")
    session_id = event['chat_name']
    question = event['prompt']

    # è·å–å½“å‰æ—¶é—´æˆ³
    request_timestamp = int(time.time())  # æˆ–è€…ä½¿ç”¨ time.time_ns() è·å–çº³ç§’çº§åˆ«çš„æ—¶é—´æˆ³
    logger.info(f'request_timestamp :{request_timestamp}')
    logger.info(f"event:{event}")
    logger.info(f"context:{context}")

    # åˆ›å»ºæ—¥å¿—ç»„å’Œæ—¥å¿—æµ
    log_group_name = '/aws/lambda/{}'.format(context.function_name)
    log_stream_name = context.aws_request_id
    client = boto3.client('logs')
    # æ¥æ”¶è§¦å‘AWS Lambdaå‡½æ•°çš„äº‹ä»¶
    logger.info('The main brain has been activated, awsğŸš€!')

    # 1. è·å–ç¯å¢ƒå˜é‡

    embedding_endpoint = os.environ.get("embedding_endpoint", "")
    aos_endpoint = os.environ.get("aos_endpoint", "")
    aos_index = os.environ.get("aos_index", "")
    aos_knn_field = os.environ.get("aos_knn_field", "")
    aos_result_num = int(os.environ.get("aos_results", ""))

    Kendra_index_id = os.environ.get("Kendra_index_id", "")
    Kendra_result_num = int(os.environ.get("Kendra_result_num", ""))
    # Opensearch_result_num = int(os.environ.get("Opensearch_result_num", ""))

    logger.info(f'embedding_endpoint : {embedding_endpoint}')
    logger.info(f'aos_endpoint : {aos_endpoint}')
    logger.info(f'aos_index : {aos_index}')
    logger.info(f'aos_knn_field : {aos_knn_field}')
    logger.info(f'aos_result_num : {aos_result_num}')
    logger.info(f'Kendra_index_id : {Kendra_index_id}')
    logger.info(f'Kendra_result_num : {Kendra_result_num}')
    
    answer = main_entry(session_id, question, embedding_endpoint, llm_endpoint, aos_endpoint, aos_index, aos_knn_field, aos_result_num,
                       Kendra_index_id, Kendra_result_num)

    # 2. return rusult

    # å¤„ç†

    # Response:
    # "id": "è®¾ç½®ä¸€ä¸ªuuid"
    # "created": "1681891998"
    # "model": "æ¨¡å‹åç§°"
    # "choices": [{"text": "æ¨¡å‹å›ç­”çš„å†…å®¹"}]
    # "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}}]

    return {
        'statusCode': 200,
        'headers': {'Content-Type': 'application/json'},
        'body': [{"id": str(uuid.uuid4()),
                             "created": request_timestamp,
                             "useTime": int(time.time()) - request_timestamp,
                             "model": "main_brain",
                             "choices":
                             [{"text": answer}],
                             "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}},
                            # {"id": uuid.uuid4(),
                            #  "created": request_timestamp,
                            #  "useTime": int(time.time()) - request_timestamp,
                            #  "model": "æ¨¡å‹åç§°",
                            #  "choices":
                            #  [{"text": "2 æ¨¡å‹å›ç­”çš„å†…å®¹"}],
                            #  "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}}
                            ]
    }

